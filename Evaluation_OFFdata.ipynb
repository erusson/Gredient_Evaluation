{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR Model Evaluation \n",
    "In this notebook, we will evaluate three models for the OCR component of Gredient. We will use data from the public dataset [Open Food Facts](https://world.openfoodfacts.org/data) (OFF), that contains images and annotations of product ingredients.\n",
    "\n",
    "We will evaluate Pytesseract, Amazon Rekognition, and Amazon Textract on their ability to correctly detect ingredients from an image. We use the OFF data to first run each model on the whole sample of data, which includes both high and low quality images. At that point, performance is fairly low over all models. So to proxy performance on high quality images, we get the top 210 scoring detections from each model and run all three models on each set of top 210 images. We average the 630 F1 scores for each model to get our final accuracy metric used for evaluation. Additionally, we record the time in seconds for each model to run and include speed in our evaluation as well. Since the performance of detections is highly dependent on image quality, we encourage our users through the interface to take better quality photos by providing them a cropping mechanism and messages indicating what a good and bad quality image looks like. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import general libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries for connecting to S3\n",
    "import boto3 \n",
    "import botocore \n",
    "from sagemaker import get_execution_role "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries for preprocessing\n",
    "import urllib\n",
    "import cv2\n",
    "from io import BytesIO\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries for OCR \n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from pytesseract import Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to S3 to read OFF data\n",
    "The OFF data was stored in Amazon's S3 bucket for ease of access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to S3 bucket to accessa OFF data\n",
    "role = get_execution_role() \n",
    "bucket = 'sagemaker-060720' \n",
    "data_key = 'evalOFFdata.csv' \n",
    "data_location = 's3://{}/{}'.format(bucket, data_key) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6576, 5)\n"
     ]
    }
   ],
   "source": [
    "# load OFF data\n",
    "eval_data = pd.read_csv(data_location)\n",
    "print(eval_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing for OCR\n",
    "- Sampling 1000 images for the evaluation of each model.\n",
    "- Since each image is saved in the form of url (link to the image), ```url_to_image``` converts the url image into an ndarray (data structure that Python can work with).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>product_name</th>\n",
       "      <th>countries_en</th>\n",
       "      <th>image_ingredients_url</th>\n",
       "      <th>ingredients_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5849</td>\n",
       "      <td>392866</td>\n",
       "      <td>Simply Lemonade</td>\n",
       "      <td>United States</td>\n",
       "      <td>https://static.openfoodfacts.org/images/produc...</td>\n",
       "      <td>PURE FILTERED WATER, SUGAR, LEMON JUICE, NATUR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5778</td>\n",
       "      <td>386955</td>\n",
       "      <td>Free-range turkey snack sticks</td>\n",
       "      <td>United States</td>\n",
       "      <td>https://static.openfoodfacts.org/images/produc...</td>\n",
       "      <td>Turkey, water, redmond seasoned salt (sea salt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4939</td>\n",
       "      <td>309534</td>\n",
       "      <td>nutricost b2</td>\n",
       "      <td>United States</td>\n",
       "      <td>https://static.openfoodfacts.org/images/produc...</td>\n",
       "      <td>supplement fac serving size: 1 capsule serving...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>159</td>\n",
       "      <td>8126</td>\n",
       "      <td>Coconut Oil</td>\n",
       "      <td>United States</td>\n",
       "      <td>https://static.openfoodfacts.org/images/produc...</td>\n",
       "      <td>coconut oil,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2038</td>\n",
       "      <td>105093</td>\n",
       "      <td>Lemon &amp; lemon zest flavored mineral water, lem...</td>\n",
       "      <td>United States</td>\n",
       "      <td>https://static.openfoodfacts.org/images/produc...</td>\n",
       "      <td>Carbonated mineral water, natural flavors.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   level_0   index                                       product_name  \\\n",
       "0     5849  392866                                    Simply Lemonade   \n",
       "1     5778  386955                     Free-range turkey snack sticks   \n",
       "2     4939  309534                                       nutricost b2   \n",
       "3      159    8126                                        Coconut Oil   \n",
       "4     2038  105093  Lemon & lemon zest flavored mineral water, lem...   \n",
       "\n",
       "    countries_en                              image_ingredients_url  \\\n",
       "0  United States  https://static.openfoodfacts.org/images/produc...   \n",
       "1  United States  https://static.openfoodfacts.org/images/produc...   \n",
       "2  United States  https://static.openfoodfacts.org/images/produc...   \n",
       "3  United States  https://static.openfoodfacts.org/images/produc...   \n",
       "4  United States  https://static.openfoodfacts.org/images/produc...   \n",
       "\n",
       "                                    ingredients_text  \n",
       "0  PURE FILTERED WATER, SUGAR, LEMON JUICE, NATUR...  \n",
       "1  Turkey, water, redmond seasoned salt (sea salt...  \n",
       "2  supplement fac serving size: 1 capsule serving...  \n",
       "3                                       coconut oil,  \n",
       "4         Carbonated mineral water, natural flavors.  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample N data for evaluation\n",
    "n=1000\n",
    "data = eval_data.sample(n, random_state=210).reset_index()\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert url to images\n",
    "def url_to_image(url):\n",
    "    resp = urllib.request.urlopen(url)\n",
    "    image = np.asarray(bytearray(resp.read()), dtype='uint8')\n",
    "    image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert image urls to np arrays with RGB\n",
    "imgs = [url_to_image(image_url) for image_url in data.image_ingredients_url]\n",
    "len(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where is the image preprocessing?\n",
    "In previous iterations of our finalized model, we implemented and evaluated five image preprocessing methods that included combinations of grayscaling, thresholding, dilating, eroding, opening, deskewing, and canny edge detection. These methods did not improve the accuracy metrics, as the OCR algorithms already include preprocessing techniques for the enhancement of each image. \n",
    "Thus, performing a combination of the said operations will result only in the increase of noise, and thus, a low accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary Functions for Cleaning Text of Ingredients in OFF dataset\n",
    "- Per given list of strings\n",
    "- Per given word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean word tokens\n",
    "def clean_word(word):\n",
    "    \n",
    "    c_word = word.lower().strip() # lowercase and remove white space\n",
    "    c_word = re.sub('[^a-zA-Z]+', '', c_word) # remove anything that's not a letter\n",
    "    if len(c_word) < 2: # remove words that are less than 2 characters\n",
    "        c_word = \"\" \n",
    "    \n",
    "    return c_word\n",
    "\n",
    "# clean list of strings \n",
    "def clean_text(text, split=True):\n",
    "    \n",
    "    if split == False: # for ocr output\n",
    "        c_text = [clean_word(w) for w in text] # already split and clean words\n",
    "        \n",
    "    else: \n",
    "        c_text = re.sub('[0-9]', ' ', text) # replace numbers with space \n",
    "        c_text = re.sub('['+string.punctuation+']', ' ', c_text) # replace punctuation with space\n",
    "        c_text = [clean_word(w) for w in c_text.split()] # split on spaces and clean words\n",
    "      \n",
    "    c_text = sorted(list(filter(None, set(c_text)))) # remove empty words and get unique values and sort\n",
    "    \n",
    "    return c_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we care about OCR performance?\n",
    "Since Gredient heavily relies on the detection of ingredients in images uploaded by users, we wanted to ensure that our OCR model had the most promising metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary Functions for Metrics of OCR only\n",
    "The following questions calculate precision, recall and F1scores for each of the OCR models we utilized. Ideally, the best model detects every word found in the ingredient label. Thus we proceed to measure precision, recall and f1 scores in this fashion to find how many of the actual ingredients in the label were succesfully detected by the OCR.\n",
    "\n",
    "- ```precision``` function:\n",
    "    - takes in a list of detected words present in the ingredients and divides them by the number of all detected words.\n",
    "- ```recall``` function:\n",
    "    - calculating recall via the detected words found in ingredients / number of words in ingredients\n",
    "- ```F1score``` function:\n",
    "    - Calculating F1 the usual way: 2\\*precision\\*recall / precision+recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision: #(detected words that are in ingredients) / #(all detected words)\n",
    "def precision(ing_lst, i, ingredients):\n",
    "    detected_words = ing_lst[i]\n",
    "    actual_ingredients = ingredients[i]\n",
    "    if len(detected_words) == 0:\n",
    "        return 0 # 0 if no detected words \n",
    "    else:\n",
    "        tp = sum([dw in actual_ingredients for dw in detected_words])\n",
    "        p = len(detected_words) # tp+fp (positives)\n",
    "        return tp/p\n",
    "    \n",
    "    \n",
    "# recall: #(detected words that are in ingredients) / #(all words in ingredients)\n",
    "def recall(ing_lst, i, ingredients):\n",
    "    detected_words = ing_lst[i]\n",
    "    actual_ingredients = ingredients[i]\n",
    "    if len(detected_words) == 0:\n",
    "        return 0 # 0 if no detected words \n",
    "    else:\n",
    "        tp = sum([dw in actual_ingredients for dw in detected_words])\n",
    "        a = len(actual_ingredients) # tp+fn (actual)\n",
    "        return tp/a\n",
    "    \n",
    "\n",
    "# f1 score: 2*precision*recall / precision+recall\n",
    "def F1score(ing_lst, i, ingredients):\n",
    "    p = precision(ing_lst, i, ingredients)\n",
    "    r = recall(ing_lst, i, ingredients)\n",
    "    if p==0 and r==0:\n",
    "        return 0\n",
    "    return (2*p*r)/(p+r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, most of the images from the OFF dataset were of low quality, and had ingredient labels that were not visible. This resulted in our OCR models performing poorly.\n",
    "Instead of dealing with the entire dataset, we created subsets of the images that were easily readbable, which had:\n",
    "- A close up of an ingredient label\n",
    "- Relatively good quality (not pixelated)\n",
    "\n",
    "Thus we selected 210 images with the highest F1 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top images, ingredients, and F1 score \n",
    "def top210(scores):\n",
    "    \n",
    "    s = np.array([scores])\n",
    "    inds = (-s).argsort()[0][:210] # top 210 detections\n",
    "\n",
    "    imgs210 = [imgs[i] for i in list(inds)] # images for top 210 detections \n",
    "    print(len(imgs210))\n",
    "\n",
    "    a_ings = [ingredients[i] for i in list(inds)] # actual ingredients for top 210 detections \n",
    "    print(len(a_ings))\n",
    "\n",
    "    print(\"Average top F1-score:\", sum([scores[i] for i in list(inds)])/210) # average F1 score for top 210 detections \n",
    "    \n",
    "    return [imgs210, a_ings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaned List of Actual Ingredients\n",
    "Here we clean the ingredients (strings) provided by the OFF dataset.\n",
    "\n",
    "Note that these are the ingredients that we will be utilizing for the evaluation of our OCR models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingredients = [clean_text(ing) for ing in data.ingredients_text] # words in ingredients \n",
    "len(ingredients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytesseract\n",
    "Call to Pytesseract OCR algorithm, assuming one block of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run pytesseract on an image\n",
    "def pytess(img) :\n",
    "    custom_oem_psm_config = r'--dpi 300 --psm 6'\n",
    "    box = pytesseract.image_to_data(img, output_type=Output.DICT, lang='eng', config=custom_oem_psm_config)\n",
    "    return box['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**--skip if already run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply pytesseract to images and return time\n",
    "start_time = time.time()\n",
    "pyt_texts = [pytess(img) for img in imgs]\n",
    "print(\"--- %s seconds ---\" % (int(time.time() - start_time)/n)) # 1.014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detected words\n",
    "pyt_ingredients = [clean_text(text, split=False) for text in pyt_texts]\n",
    "len(pyt_ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output of tesseract\n",
    "pyt_output = pd.DataFrame({'detected':pyt_ingredients})\n",
    "pyt_output.to_csv('pyt_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**--**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get ingredients that were detected, clean them, calculate F1 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['contains', 'filtered', 'flavors', 'juice', 'lemon', 'natural', 'pure', 'sugar', 'water'], ['coe', 'igen', 'ingredients', 'musa', 'polis', 'rlefobeaess', 'tas'], ['are', 'es', 'fa', 'so', 'supplement', 'tren'], ['above', 'acon', 'aeohe', 'ane', 'approximatey', 'asmoke', 'ay', 'becomes', 'bho', 'co', 'coconut', 'cool', 'cooonuror', 'dry', 'en', 'extreme', 'facts', 'fam', 'hae', 'heat', 'ina', 'ingredients', 'lguid', 'ma', 'of', 'oistrcuten', 'ol', 'ona', 'oretvedu', 'plageana', 'point', 'refined', 'sou', 'srst', 'store', 'the', 'wea', 'xnocen'], ['']]\n",
      "1000\n",
      "Average F1-score: 0.31855246325287856\n"
     ]
    }
   ],
   "source": [
    "# load saved data in csv\n",
    "pyt_data = pd.read_csv('pyt_output.csv')\n",
    "\n",
    "# get detected ingredients\n",
    "pyt_ingredients = [[re.sub('[^a-zA-Z]+', '', e) for e in l.split(\",\")] for l in pyt_data.detected]\n",
    "\n",
    "# peak at detected ingredients\n",
    "print(pyt_ingredients[:5])\n",
    "print(len(pyt_ingredients))\n",
    "\n",
    "# get F1 scores for pytesseract detections\n",
    "pyt_scores = [F1score(pyt_ingredients, i, ingredients) for i in range(n)]\n",
    "\n",
    "print(\"Average F1-score:\", sum(pyt_scores)/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n",
      "210\n",
      "Average top F1-score: 0.8488069532533237\n"
     ]
    }
   ],
   "source": [
    "# top 210 images and actual ingredients\n",
    "pyt_imgs, a_pyt_ings = top210(pyt_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rekognition\n",
    "Call to AWS Rekognition for the OCR of ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate rekognition object\n",
    "rek_client=boto3.client('rekognition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run rekognition on an image\n",
    "def rekogn(img):\n",
    "    pil_img = Image.fromarray(img)\n",
    "    buff = BytesIO()\n",
    "    pil_img.save(buff, format=\"JPEG\")\n",
    "    img_bytes = buff.getvalue()\n",
    "    rek_text = rek_client.detect_text(Image={\"Bytes\":img_bytes})\n",
    "    return rek_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**--skip if already run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply rekognition to images and return time\n",
    "start_time = time.time()\n",
    "rek_texts = [rekogn(img) for img in imgs]\n",
    "print(\"--- %s seconds ---\" % (int(time.time() - start_time)/n)) # 4.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detected words\n",
    "rek_words = [[text['DetectedText'] if text['Type']=='WORD' else \"\" for text in texts['TextDetections']] for texts in rek_texts]\n",
    "\n",
    "rek_ingredients = [clean_text(text, split=False) for text in rek_words] # detected words\n",
    "\n",
    "len(rek_ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output of rekognition\n",
    "rek_output = pd.DataFrame({'detected':rek_ingredients})\n",
    "rek_output.to_csv('rek_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**--**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again read the detections from Rekognition algorithm, clean detections, and calculate F1 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['contains', 'filtered', 'flavors', 'juice', 'lemon', 'natural', 'pure', 'sugar', 'water'], ['acid', 'agen', 'beef', 'black', 'casing', 'celery', 'co', 'coriander', 'cutured', 'encapsutated', 'feeange', 'ic', 'in', 'induding', 'ingredients', 'mustard', 'onion', 'owder', 'paprikatumeric', 'pasley', 'pepper', 'powder', 'redmond', 'sal', 'salt', 'sat', 'sea', 'seasoned', 'spices', 'tc', 'turkey', 'water'], ['acid', 'anin', 'aonbunt', 'caicium', 'capsule', 'container', 'daly', 'due', 'established', 'fac', 'magnesium', 'mg', 'ngridenesgelasin', 'noe', 'ohee', 'per', 'ribofavin', 'riceflout', 'sarios', 'serving', 'sevig', 'sico', 'size', 'stearave', 'supplement', 'vale', 'veortable'], ['ainer', 'and', 'approximately', 'away', 'best', 'by', 'cincinnati', 'co', 'coconut', 'cool', 'distributed', 'dry', 'extreme', 'facts', 'for', 'forbaking', 'free', 'from', 'gluten', 'has', 'heat', 'in', 'ingredients', 'is', 'kroger', 'mediumhigh', 'of', 'ohio', 'oil', 'ol', 'or', 'over', 'place', 'point', 'refined', 'sauteing', 'smoke', 'stirfrg', 'store', 'tbsp', 'the'], ['carbonated', 'distririited', 'flavors', 'ingredients', 'mineral', 'natural', 'water']]\n",
      "1000\n",
      "Average F1-score: 0.501141273843894\n"
     ]
    }
   ],
   "source": [
    "# load saved data in csv\n",
    "rek_data = pd.read_csv('rek_output.csv')\n",
    "\n",
    "# get detected ingredients\n",
    "rek_ingredients = [[re.sub('[^a-zA-Z]+', '', e) for e in l.split(\",\")] for l in rek_data.detected]\n",
    "\n",
    "# peak at detected ingredients\n",
    "print(rek_ingredients[:5])\n",
    "print(len(rek_ingredients))\n",
    "\n",
    "# get F1 scores for rekognition detections\n",
    "rek_scores = [F1score(rek_ingredients, i, ingredients) for i in range(n)]\n",
    "\n",
    "print(\"Average F1-score:\", sum(rek_scores)/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n",
      "210\n",
      "Average top F1-score: 0.9244284208177813\n"
     ]
    }
   ],
   "source": [
    "# top 210 images and actual ingredients\n",
    "rek_imgs, a_rek_ings = top210(rek_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textract\n",
    "Call AWS Textract to make predictions on ingredient list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate textract object\n",
    "tex_client = boto3.client('textract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run textract on an image\n",
    "def textract(img):\n",
    "    pil_img = Image.fromarray(img)\n",
    "    buff = BytesIO()\n",
    "    pil_img.save(buff, format=\"JPEG\")\n",
    "    img_bytes = buff.getvalue()\n",
    "    tex_text = tex_client.detect_document_text(Document={\"Bytes\":img_bytes})\n",
    "    return tex_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**--skip if already run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply textract to images and return time\n",
    "start_time = time.time()\n",
    "tex_texts = [textract(img) for img in imgs]\n",
    "print(\"--- %s seconds ---\" % (int(time.time() - start_time)/n)) # 1.354"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detected words\n",
    "tex_words = [[text['Text'] if text['BlockType']=='WORD' else \"\" for text in texts['Blocks']] for texts in tex_texts]\n",
    "\n",
    "tex_ingredients = [clean_text(text, split=False) for text in tex_words] # detected words\n",
    "\n",
    "len(tex_ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output of textract\n",
    "tex_output = pd.DataFrame({'detected':tex_ingredients})\n",
    "tex_output.to_csv('tex_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**--**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean detections, calculate F1 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['contains', 'filtered', 'juice', 'lemon', 'natural', 'pure', 'rlavors', 'sugar', 'water'], ['bas', 'bes', 'bououunteyuded', 'bpmo', 'daepoepow', 'epwode', 'espauoseaspuoupey', 'fuseyuebe', 'il', 'jjemfaxnebueseay', 'jpequ', 'laddeo', 'lapueuoobupnpujsaodses', 'lase', 'lepopangno', 'per', 'perensdeoue', 'pjegsnw', 'poeon', 'sinbiobuon', 'uouo'], ['amount', 'and', 'blue', 'cakcion', 'capsule', 'container', 'established', 'fac', 'flour', 'gelatin', 'ingredonts', 'magnesiu', 'ng', 'no', 'oe', 'pe', 'per', 'ribotain', 'rice', 'route', 'seeing', 'serving', 'shearate', 'sine', 'son', 'supplement', 'truly', 'veoetable', 'veri'], ['above', 'and', 'aner', 'approximately', 'away', 'baking', 'becomes', 'below', 'best', 'by', 'cincinnati', 'co', 'coconut', 'cool', 'dally', 'distributed', 'dry', 'extreme', 'facts', 'fand', 'for', 'free', 'from', 'gluten', 'has', 'heat', 'in', 'ingredients', 'is', 'kroger', 'liquid', 'mediumhigh', 'of', 'ohi', 'oil', 'ol', 'or', 'over', 'place', 'point', 'refined', 'sauteing', 'smoke', 'solid', 'store', 'str', 'tbsp', 'the', 'values'], ['carbonated', 'distririited', 'flavors', 'ingredients', 'mineral', 'natural', 'water']]\n",
      "1000\n",
      "Average F1-score: 0.4960002357468954\n"
     ]
    }
   ],
   "source": [
    "# load saved data in csv\n",
    "tex_data = pd.read_csv('tex_output.csv')\n",
    "\n",
    "# get detected ingredients\n",
    "tex_ingredients = [[re.sub('[^a-zA-Z]+', '', e) for e in l.split(\",\")] for l in tex_data.detected]\n",
    "\n",
    "# peak at detected ingredients\n",
    "print(tex_ingredients[:5])\n",
    "print(len(tex_ingredients))\n",
    "\n",
    "# get F1 scores for textract detections\n",
    "tex_scores = [F1score(tex_ingredients, i, ingredients) for i in range(n)]\n",
    "\n",
    "print(\"Average F1-score:\", sum(tex_scores)/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n",
      "210\n",
      "Average top F1-score: 0.9197823584411816\n"
     ]
    }
   ],
   "source": [
    "# top 210 images and actual ingredients\n",
    "tex_imgs, a_tex_ings = top210(tex_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Quality Images\n",
    "\n",
    "We proceed to evaluate each model on all three sets of \"high-quality\" images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run each model on top images\n",
    "def run_all(imgs, n):\n",
    "    \n",
    "    # apply pytesseract to images and return time\n",
    "    start_time = time.time()\n",
    "    p_texts = [pytess(img) for img in imgs]\n",
    "    p_time = int(time.time() - start_time)/n\n",
    "    print(\"--- pyt %s seconds ---\" % (p_time))\n",
    "    p_ingredients = [clean_text(text, split=False) for text in p_texts]\n",
    "    print(len(p_ingredients))\n",
    "    \n",
    "    # apply rekognition to images and return time\n",
    "    start_time = time.time()\n",
    "    r_texts = [rekogn(img) for img in imgs]\n",
    "    r_time = int(time.time() - start_time)/n\n",
    "    print(\"--- rek %s seconds ---\" % (r_time)) \n",
    "    r_words = [[text['DetectedText'] if text['Type']=='WORD' else \"\" for text in texts['TextDetections']] for texts in r_texts]\n",
    "    r_ingredients = [clean_text(text, split=False) for text in r_words] # detected words\n",
    "    print(len(r_ingredients))\n",
    "    \n",
    "    # apply rekognition to images and return time\n",
    "    start_time = time.time()\n",
    "    t_texts = [textract(img) for img in imgs]\n",
    "    t_time = int(time.time() - start_time)/n\n",
    "    print(\"--- tex %s seconds ---\" % (t_time))\n",
    "    t_words = [[text['Text'] if text['BlockType']=='WORD' else \"\" for text in texts['Blocks']] for texts in t_texts]\n",
    "    t_ingredients = [clean_text(text, split=False) for text in t_words] # detected words\n",
    "    print(len(t_ingredients))\n",
    "    \n",
    "    return [[p_ingredients, r_ingredients, t_ingredients],[p_time, r_time, t_time]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top scores for each model on a set of top detections\n",
    "def top_scores(top_detections, top_ingredients):\n",
    "    \n",
    "    top_s = [[F1score(td, i, ti) for i in range(210)] for td,ti in zip(top_detections,[top_ingredients]*3)]\n",
    "\n",
    "    print(\"Average F1-scores (pyt):\", sum(top_s[0])/210)\n",
    "    print(\"Average F1-scores (rek):\", sum(top_s[1])/210)\n",
    "    print(\"Average F1-scores (tex):\", sum(top_s[2])/210)\n",
    "    \n",
    "    return top_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores for Top Pytesseract Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- pyt 0.4095238095238095 seconds ---\n",
      "210\n",
      "--- rek 4.223809523809524 seconds ---\n",
      "210\n",
      "--- tex 1.180952380952381 seconds ---\n",
      "210\n"
     ]
    }
   ],
   "source": [
    "top_pyt = run_all(pyt_imgs, 210)\n",
    "top_pyt_ingredients = top_pyt[0]\n",
    "top_pyt_time = top_pyt[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1-scores (pyt): 0.8483084278219671\n",
      "Average F1-scores (rek): 0.8435603771332821\n",
      "Average F1-scores (tex): 0.8612136523989301\n"
     ]
    }
   ],
   "source": [
    "top_pyt_scores = top_scores(top_pyt_ingredients,a_pyt_ings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores for Top Rekognition Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- pyt 0.37142857142857144 seconds ---\n",
      "210\n",
      "--- rek 3.7 seconds ---\n",
      "210\n",
      "--- tex 1.2285714285714286 seconds ---\n",
      "210\n"
     ]
    }
   ],
   "source": [
    "top_rek = run_all(rek_imgs, 210)\n",
    "top_rek_ingredients = top_rek[0]\n",
    "top_rek_time = top_rek[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1-scores (pyt): 0.6465259060446534\n",
      "Average F1-scores (rek): 0.9234937130446207\n",
      "Average F1-scores (tex): 0.8222205873419498\n"
     ]
    }
   ],
   "source": [
    "top_rek_scores = top_scores(top_rek_ingredients,a_rek_ings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores for Top Textract Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- pyt 0.40476190476190477 seconds ---\n",
      "210\n",
      "--- rek 3.604761904761905 seconds ---\n",
      "210\n",
      "--- tex 1.0904761904761904 seconds ---\n",
      "210\n"
     ]
    }
   ],
   "source": [
    "top_tex = run_all(tex_imgs, 210)\n",
    "top_tex_ingredients = top_tex[0]\n",
    "top_tex_time = top_tex[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1-scores (pyt): 0.6907304846374634\n",
      "Average F1-scores (rek): 0.8651720762650041\n",
      "Average F1-scores (tex): 0.9096800807703677\n"
     ]
    }
   ],
   "source": [
    "top_tex_scores = top_scores(top_tex_ingredients,a_tex_ings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Scores for All Top Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytesseract 0.728521606168028\n",
      "Rekognition 0.8774087221476357\n",
      "Textract 0.8643714401704159\n"
     ]
    }
   ],
   "source": [
    "model = ['Pytesseract', 'Rekognition', 'Textract']\n",
    "\n",
    "for i in range(3):\n",
    "    print(model[i], (sum(top_pyt_scores[i]) + sum(top_rek_scores[i]) + sum(top_tex_scores[i])) / (3*210))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytesseract 0.3952380952380952\n",
      "Rekognition 3.842857142857143\n",
      "Textract 1.1666666666666667\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(model[i], (top_pyt_time[i] + top_rek_time[i] + top_tex_time[i])/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "- Pytesseract has an average F1 score of 0.728 and speed of 0.4 seconds/image\n",
    "    - 0.84881 for top 210 \n",
    "    - 0.31855 for full sample\n",
    "- Rekognition has an average F1 score of 0.878 and speed of 3.8 seconds/image  \n",
    "    - 0.92443 for top 210 \n",
    "    - 0.50114 for full sample \n",
    "- Textract has an average F1 score of 0.864 and speed of 1.2 seconds/image  \n",
    "    - 0.91978 for top 210 \n",
    "    - 0.49600 for full sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_ocr_new",
   "language": "python",
   "name": "conda_ocr_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
